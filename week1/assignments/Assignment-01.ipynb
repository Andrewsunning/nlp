{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {网络欺诈识别\\新闻情感分类\\电子商务的商品推荐}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {  \n",
    "（1）上传自己的代码，同时下载和查看别人的代码  \n",
    "（2）辅助我们进行代码开发，更容易的debug等  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "概率模型是一个概率分布函数或密度函数的集合，主要可以分为参数模型、无参数模型：  \n",
    "参数模型的求解主要是通过参数估计，一般使用极大似然估计；  \n",
    "无参数模型的构建方法主要是核方法和近邻法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1. 图像识别识别出属于不同类别的概率（softmax转化）；  \n",
    "2. 朴素贝叶斯对是否为垃圾邮件的识别等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "主要是因为现实生活中充满着不确定性和随机性，而概率可以看作是不确定性的逻辑扩展，可以很好的表征这些不确定性，而与之相对的，基于解析和模式识别的方法却很难处理这种不确定性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "实质上是一个用来计算某句话出现概率的概率模型，其有一个有限集合V（表示词典）和词的概率分布P构成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "利用语言模型进行文本分类；  \n",
    "构建线上聊天机器人等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "仅考虑当前token出现概率的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "优点：  \n",
    "（1）计算简单，容易理解  \n",
    "（2）模型构建快  \n",
    "缺点：  \n",
    "（1）没有考虑上下文  \n",
    "（2）模型性能较差  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "同时考虑相邻两个词关联性的语言模型，即计算给定当前词的条件下，前一个单词出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 定义rules（解析树形式）\n",
    "chatbot = '''\n",
    "chatbot = 寒暄 自我介绍 询问 相关问题 结尾\n",
    "寒暄 = 名字 称谓 打招呼 | 打招呼\n",
    "自我介绍 = 我是客服 代号 ，\n",
    "名字 = 小李 | 老王 | 张三 | 李四\n",
    "称谓 = 人称\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好， | 您好，\n",
    "代号 = 小M | 小i | 小a\n",
    "询问 = 请问您需要问 | 您需要问\n",
    "相关问题 = 快递 | 优惠活动 | 尺码 | 颜色\n",
    "结尾 = 问题吗？\n",
    "'''\n",
    "\n",
    "human = \"\"\"\n",
    "human = 打招呼 陈述 相关问题 结尾\n",
    "打招呼 = 你好， | 您好，\n",
    "陈述 = 我想询问 | 我想知道\n",
    "相关问题 = 快递 | 优惠活动 | 尺码 | 颜色\n",
    "结尾 = 的信息。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammar_rules: dict, target: str):\n",
    "    if target in grammar_rules:\n",
    "        candidates = grammar_rules[target]\n",
    "        candidate = random.choice(candidates)\n",
    "        return ' '.join(generate(grammar_rules, target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target\n",
    "\n",
    "def get_generation_by_gram(grammar_str: str, target, stmt_split='=', or_split='|'):\n",
    "    rules = dict()\n",
    "    for line in grammar_str.splitlines():\n",
    "        if not line: continue    # 跳过空行\n",
    "        stmt, expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = [x.strip() for x in expr.split(or_split)]\n",
    "    generated = generate(rules, target)\n",
    "    return generated\n",
    "# print(get_generation_by_gram(chatbot, target='chatbot'))\n",
    "# print(get_generation_by_gram(human, target='human'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入要生成对话的对数：6\n",
      "1\n",
      "您好， 我是客服 小i ， 请问您需要问 优惠活动 问题吗？\n",
      "您好， 我想知道 优惠活动 的信息。\n",
      "\n",
      "2\n",
      "您好， 我是客服 小a ， 请问您需要问 颜色 问题吗？\n",
      "你好， 我想知道 快递 的信息。\n",
      "\n",
      "3\n",
      "李四 先生 您好， 我是客服 小i ， 您需要问 颜色 问题吗？\n",
      "您好， 我想知道 优惠活动 的信息。\n",
      "\n",
      "4\n",
      "您好， 我是客服 小M ， 您需要问 优惠活动 问题吗？\n",
      "您好， 我想知道 尺码 的信息。\n",
      "\n",
      "5\n",
      "李四 先生 您好， 我是客服 小i ， 请问您需要问 颜色 问题吗？\n",
      "您好， 我想询问 优惠活动 的信息。\n",
      "\n",
      "6\n",
      "你好， 我是客服 小M ， 您需要问 尺码 问题吗？\n",
      "你好， 我想知道 颜色 的信息。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_n(n):\n",
    "    # you code here \n",
    "    for i in range(n):\n",
    "        print(i+1)\n",
    "        print(get_generation_by_gram(chatbot, target='chatbot'))\n",
    "        print(get_generation_by_gram(human, target='human'))\n",
    "        print()\n",
    "n = int(input(\"请输入要生成对话的对数：\"))\n",
    "generate_n(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.文本清洗，只保留保留txt文本中的汉字和标点符号\n",
    "# 2. jieba分词处理\n",
    "# 3. 构建新的2-Gram语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本清洗\n",
    "import re\n",
    "\n",
    "PATH = '../data/2_train.txt'\n",
    "corpus_path = '../data/2_train_processed.txt'\n",
    "\n",
    "FILE = open(PATH).read()\n",
    "with open(corpus_path,mode='w') as f_w:    # 注意：写文件时，mode一定要设为‘w’\n",
    "    for n in re.findall(r'[\\u4e00-\\u9fff]+',FILE):    # FILE是一个长字符串\n",
    "        f_w.write(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/z9/yzvzfk354vgdpmx2y24y82m00000gn/T/jieba.cache\n",
      "Loading model cost 0.975 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74259\n",
      "[('保险', 5005), ('的', 3220), ('人寿保险', 2962), ('什么', 2675), ('吗', 2479), ('是', 2344), ('我', 2053), ('是否', 1862), ('可以', 1704), ('健康', 1513)]\n",
      "[('健康保险', 1347), ('什么是', 1152), ('保险是否', 975), ('我的', 726), ('残疾保险', 658), ('房主保险', 602), ('我可以', 530), ('保险吗', 511), ('是否覆盖', 504), ('家庭保险', 440)]\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "import jieba\n",
    "\n",
    "FILE_2 = open(corpus_path).read()\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "TOKENS = cut(FILE_2)\n",
    "print(len(TOKENS))\n",
    "\n",
    "from collections import Counter\n",
    "words_count = Counter(TOKENS)\n",
    "print(words_count.most_common(10))\n",
    "\n",
    "_2_gram_words = [TOKENS[i]+TOKENS[i+1] for i in range(len(TOKENS)-1)]\n",
    "_2_gram_words_count = Counter(_2_gram_words)\n",
    "print(_2_gram_words_count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_word_count(word: str, wc: dict):\n",
    "    if word in wc: return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]\n",
    "    \n",
    "def two_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    probability = 1.0\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        _2_gram_count = get_gram_word_count(word+next_word, _2_gram_words_count)\n",
    "        _1_gram_count = get_gram_word_count(next_word,words_count)\n",
    "        pro = _2_gram_count / _1_gram_count\n",
    "        probability*=pro\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4252531897089152e-16"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model(\"法律要求残疾保险吗债权人可以在死后人寿保险吗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小李 小朋友 您好， 我是客服 小a ， 请问您需要问 优惠活动 问题吗？'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_best(n, grammar_str, gram_model): # you code here\n",
    "    \"\"\"\n",
    "    n:int, 要生成的句子数\n",
    "    grammar_str： str, 生成句子的语法树\n",
    "    gram_model:func, 语言模型函数\n",
    "    \n",
    "    return: 出现概率最高的一句话\n",
    "    \"\"\"\n",
    "    sentences_prob = []\n",
    "    for i in range(n):\n",
    "        sentence = get_generation_by_gram(grammar_str, 'chatbot')\n",
    "        prob = two_gram_model(sentence)\n",
    "        sentences_prob.append((prob, sentence))\n",
    "    return sorted(sentences_prob, key=lambda x:x[0], reverse=True)[-1][-1]\n",
    "\n",
    "print(generate_best(5, chatbot, two_gram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "只考虑了相邻两个token的上下文，忽略了其他token与当前token的语义关系；  \n",
    "可以考虑使用3-gram或者使用LSTM模型，attention机制等等来改进"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
